{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5491b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import tqdm\n",
    "\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16fcb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 21:35:11.567400: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-05-30 21:35:11.567484: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cs229-vm-vm): /proc/driver/nvidia/version does not exist\n",
      "2022-05-30 21:35:11.569480: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947d0355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "pose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi')\n",
    "sys.path.append(pose_sample_rpi_path)\n",
    "\n",
    "# Load MoveNet Thunder model\n",
    "import utils\n",
    "from data import BodyPart\n",
    "from ml import Movenet\n",
    "movenet = Movenet('movenet_thunder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e4b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/shellygoel2324/data_clean.csv'\n",
    "labels_path = '/home/shellygoel2324/processedLabels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d37956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pose_landmarks(csv_path, labels_path):\n",
    "    \"\"\"Loads a CSV created by MoveNetPreprocessor.\n",
    "    Returns:\n",
    "        X: Detected landmark coordinates and scores of shape (N, 17 * 3)\n",
    "        y: Ground truth labels of shape (N, label_count)\n",
    "        classes: The list of all class names found in the dataset\n",
    "        dataframe: The CSV loaded as a Pandas dataframe features (X) and ground\n",
    "        truth labels (y) to use later to train a pose classification model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV file\n",
    "    dataframe = pd.read_csv(csv_path)\n",
    "    labels = pd.read_csv(labels_path, header=None)\n",
    "    df_to_process = dataframe.copy()\n",
    "\n",
    "    # Drop the file_name columns as you don't need it during training.\n",
    "    df_to_process.drop(columns=['file_name'], inplace=True)\n",
    "\n",
    "    # Extract the list of class names\n",
    "    df_to_process.pop('class_name')\n",
    "    df_to_process.pop('class_no')\n",
    "\n",
    "    # Extract the labels\n",
    "    y = labels\n",
    "    classes = range(8)\n",
    "\n",
    "    # Convert the input features and labels into the correct format for training.\n",
    "    X = df_to_process.astype('float64')\n",
    "    y = keras.utils.to_categorical(y)\n",
    "    \n",
    "    return X, y, classes, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93bd23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10255, 51)\n"
     ]
    }
   ],
   "source": [
    "# Load the train data\n",
    "X, y, class_names, _ = load_pose_landmarks(data_path, labels_path)\n",
    "\n",
    "# 80/10/10\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b4835b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DISTRIBUTION\n",
      "1: 0.50931253047294\n",
      "2: 0.24280838615309605\n",
      "3: 0.11038517796196977\n",
      "4: 0.04475865431496831\n",
      "5: 0.02340321794246709\n",
      "6: 0.008776206728425159\n",
      "7: 0.060555826426133594\n",
      "\n",
      "TEST DISTRIBUTION\n",
      "0: 0.0\n",
      "1: 0.5195007800312013\n",
      "2: 0.24726989079563183\n",
      "3: 0.09750390015600624\n",
      "4: 0.039781591263650544\n",
      "5: 0.0358814352574103\n",
      "6: 0.0062402496099844\n",
      "7: 0.05382215288611544\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN DISTRIBUTION\")\n",
    "\n",
    "sample_dist = []\n",
    "\n",
    "'''\n",
    "for i in range(0,8):\n",
    "    \n",
    "    num_i = 0\n",
    "    for ind, sample in enumerate(y_train):\n",
    "            if tf.argmax(sample) == i:\n",
    "                if i ==0:\n",
    "                    print(ind)\n",
    "                    toRemove = ind\n",
    "                \n",
    "                num_i+=1\n",
    "    dist = num_i/len(y_train)\n",
    "    print(f\"{i}: {dist}\")\n",
    "\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "y_train = np.delete(y_train,toRemove,axis = 0)\n",
    "X_train = np.delete(X_train,toRemove+1,axis = 0)\n",
    "\n",
    "'''\n",
    "\n",
    "for i in range(1,8):\n",
    "    \n",
    "    num_i = 0\n",
    "    for ind, sample in enumerate(y_train):\n",
    "            if tf.argmax(sample) == i:\n",
    "                if i ==0:\n",
    "                    print(\"HERE\")\n",
    "                \n",
    "                num_i+=1\n",
    "    dist = num_i/len(y_train)\n",
    "    print(f\"{i}: {dist}\")\n",
    "    sample_dist.append(dist)\n",
    "\n",
    "    \n",
    "print(\"\\nTEST DISTRIBUTION\")\n",
    "for i in range(0,8):\n",
    "    \n",
    "    num_i = 0\n",
    "    for sample in y_test:\n",
    "            if tf.argmax(sample) == i:\n",
    "                if i ==0:\n",
    "                    print(sample)\n",
    "                num_i+=1\n",
    "\n",
    "    dist = num_i/len(y_test)\n",
    "    print(f\"{i}: {dist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "74657710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_dist = sample_dist/np.linalg.norm(sample_dist)\n",
    "weight_balanced= [1/s for s in sample_dist]\n",
    "\n",
    "sample_weights = weight_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "348958a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_point(landmarks, left_bodypart, right_bodypart):\n",
    "    \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n",
    "    \n",
    "    left = tf.gather(landmarks, left_bodypart.value, axis=1)\n",
    "    right = tf.gather(landmarks, right_bodypart.value, axis=1)\n",
    "    center = left * 0.5 + right * 0.5\n",
    "    return center\n",
    "\n",
    "\n",
    "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
    "    \"\"\"Calculates pose size.\n",
    "\n",
    "        It is the maximum of two values:\n",
    "        * Torso size multiplied by `torso_size_multiplier`\n",
    "        * Maximum distance from pose center to any pose landmark\n",
    "    \"\"\"\n",
    "    # Hips center\n",
    "    hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "\n",
    "    # Shoulders center\n",
    "    shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n",
    "                                      BodyPart.RIGHT_SHOULDER)\n",
    "\n",
    "    # Torso size as the minimum body size\n",
    "    torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
    "\n",
    "    # Pose center\n",
    "    pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                     BodyPart.RIGHT_HIP)\n",
    "    pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to\n",
    "    # perform substraction\n",
    "    pose_center_new = tf.broadcast_to(pose_center_new,\n",
    "                                    [tf.size(landmarks) // (17*2), 17, 2])\n",
    "\n",
    "    # Dist to pose center\n",
    "    d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n",
    "                name=\"dist_to_pose_center\")\n",
    "    # Max dist to pose center\n",
    "    max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
    "\n",
    "    # Normalize scale\n",
    "    pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
    "\n",
    "    return pose_size\n",
    "\n",
    "\n",
    "def normalize_pose_landmarks(landmarks):\n",
    "    \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
    "        scaling it to a constant pose size.\n",
    "    \"\"\"\n",
    "    # Move landmarks so that the pose center becomes (0,0)\n",
    "    pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "    pose_center = tf.expand_dims(pose_center, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to perform\n",
    "    # substraction\n",
    "    pose_center = tf.broadcast_to(pose_center, \n",
    "                                [tf.size(landmarks) // (17*2), 17, 2])\n",
    "    landmarks = landmarks - pose_center\n",
    "\n",
    "    # Scale the landmarks to a constant pose size\n",
    "    pose_size = get_pose_size(landmarks)\n",
    "    landmarks /= pose_size\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def landmarks_to_embedding(landmarks_and_scores):\n",
    "    \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
    "    # Reshape the flat input into a matrix with shape=(17, 3)\n",
    "    reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
    "\n",
    "    # Normalize landmarks 2D\n",
    "    landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])\n",
    "\n",
    "    # Flatten the normalized landmark coordinates into a vector\n",
    "    embedding = keras.layers.Flatten()(landmarks)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def no_normalization(landmarks_and_scores):\n",
    "    \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
    "    # Reshape the flat input into a matrix with shape=(17, 3)\n",
    "    landmarks = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
    "\n",
    "    # Flatten the landmark coordinates into a vector\n",
    "    embedding = keras.layers.Flatten()(landmarks[:, :, :2])\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cea2f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"Plots the confusion matrix.\"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=55)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "              horizontalalignment=\"center\",\n",
    "              color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e2eea",
   "metadata": {},
   "source": [
    "## No normalizing at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9ffd45da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 51)]              0         \n",
      "                                                                 \n",
      " reshape_6 (Reshape)         (None, 17, 3)             0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem_6   (None, 17, 2)            0         \n",
      " (SlicingOpLambda)                                               \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 34)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               4480      \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,256\n",
      "Trainable params: 13,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "inputs = tf.keras.Input(shape=(51))\n",
    "embedding = no_normalization(inputs)\n",
    "\n",
    "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2aa8df1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9634309783649244, 4.118473895582329, 9.059187279151944, 22.342047930283222, 42.729166666666664, 113.94444444444444, 16.513687600644122]\n",
      "[0.01567794 0.03288589 0.07233735 0.1784006  0.34119116 0.90984309\n",
      " 0.13186132]\n"
     ]
    }
   ],
   "source": [
    "print(sample_weights)\n",
    "\n",
    "\n",
    "sample_weights = sample_weights/np.linalg.norm(sample_weights)\n",
    "\n",
    "print(sample_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "117a48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {}\n",
    "class_weights[0] = 0\n",
    "for i in range(1,8):\n",
    "    class_weights[i] = sample_weights[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "baf0903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0.01567793943728298, 2: 0.032885894650975495, 3: 0.07233734777467225, 4: 0.17840060496934418, 5: 0.34119115700387076, 6: 0.9098430853436554, 7: 0.1318613167164718}\n"
     ]
    }
   ],
   "source": [
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f2ef925c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#USING FOCAL LOSS\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "fl = tfa.losses.SigmoidFocalCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3343445",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "636/641 [============================>.] - ETA: 0s - loss: 0.0602 - tp: 653.0000 - fp: 2867.0000 - tn: 77339.0000 - fn: 10805.0000 - accuracy: 0.8508 - precision: 0.1855 - recall: 0.0570 - auc: 0.5641 - prc: 0.1532\n",
      "Epoch 1: val_accuracy improved from -inf to 0.87500, saving model to weights.best.801010split\n",
      "INFO:tensorflow:Assets written to: weights.best.801010split/assets\n",
      "641/641 [==============================] - 6s 6ms/step - loss: 0.0599 - tp: 653.0000 - fp: 2868.0000 - tn: 77891.0000 - fn: 10884.0000 - accuracy: 0.8510 - precision: 0.1855 - recall: 0.0566 - auc: 0.5639 - prc: 0.1531 - val_loss: 0.4105 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 8974.0000 - val_fn: 1282.0000 - val_accuracy: 0.8750 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4078 - val_prc: 0.0963\n",
      "Epoch 2/200\n",
      "638/641 [============================>.] - ETA: 0s - loss: 0.0234 - tp: 12.0000 - fp: 68.0000 - tn: 71388.0000 - fn: 10196.0000 - accuracy: 0.8743 - precision: 0.1500 - recall: 0.0012 - auc: 0.4917 - prc: 0.1253\n",
      "Epoch 2: val_accuracy did not improve from 0.87500\n",
      "641/641 [==============================] - 2s 3ms/step - loss: 0.0234 - tp: 12.0000 - fp: 68.0000 - tn: 71717.0000 - fn: 10243.0000 - accuracy: 0.8743 - precision: 0.1500 - recall: 0.0012 - auc: 0.4918 - prc: 0.1254 - val_loss: 0.4022 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 8974.0000 - val_fn: 1282.0000 - val_accuracy: 0.8750 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4075 - val_prc: 0.0963\n",
      "Epoch 3/200\n",
      "632/641 [============================>.] - ETA: 0s - loss: 0.0219 - tp: 3.0000 - fp: 27.0000 - tn: 70757.0000 - fn: 10109.0000 - accuracy: 0.8747 - precision: 0.1000 - recall: 2.9668e-04 - auc: 0.5434 - prc: 0.1335\n",
      "Epoch 3: val_accuracy did not improve from 0.87500\n",
      "641/641 [==============================] - 2s 3ms/step - loss: 0.0220 - tp: 4.0000 - fp: 27.0000 - tn: 71758.0000 - fn: 10251.0000 - accuracy: 0.8747 - precision: 0.1290 - recall: 3.9005e-04 - auc: 0.5442 - prc: 0.1337 - val_loss: 0.3682 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 8974.0000 - val_fn: 1282.0000 - val_accuracy: 0.8750 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6760 - val_prc: 0.1790\n",
      "Epoch 4/200\n",
      "315/641 [=============>................] - ETA: 0s - loss: 0.0216 - tp: 1.0000 - fp: 13.0000 - tn: 35267.0000 - fn: 5039.0000 - accuracy: 0.8747 - precision: 0.0714 - recall: 1.9841e-04 - auc: 0.5745 - prc: 0.1375"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tfa.losses.SigmoidFocalCrossEntropy(),\n",
    "    metrics=METRICS\n",
    ")\n",
    "\n",
    "# Add a checkpoint callback to store the checkpoint that has the highest\n",
    "# validation accuracy.\n",
    "#checkpoint_path = \"weights.best.hdf5\"\n",
    "\n",
    "checkpoint_path = \"weights.best.801010split\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                              patience=20)\n",
    "\n",
    "# Start training\n",
    "\n",
    "history = model.fit(X_train, y_train,class_weight= class_weights,\n",
    "                    epochs=200,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d2ebc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the training history to see whether you're overfitting.\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['TRAIN', 'VAL'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a445884c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.metrics.TruePositives at 0x7efd000974d0>,\n",
       " <keras.metrics.FalsePositives at 0x7efd001e7dd0>,\n",
       " <keras.metrics.TrueNegatives at 0x7efd000f61d0>,\n",
       " <keras.metrics.FalseNegatives at 0x7efd0020ddd0>,\n",
       " <keras.metrics.BinaryAccuracy at 0x7efcf2ecf610>,\n",
       " <keras.metrics.Precision at 0x7efd001e2b50>,\n",
       " <keras.metrics.Recall at 0x7efda4321f90>,\n",
       " <keras.metrics.AUC at 0x7efcf2c91ed0>,\n",
       " <keras.metrics.AUC at 0x7efcf2c4b5d0>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "20634a80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 2ms/step - loss: 0.3493 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 8974.0000 - fn: 1282.0000 - accuracy: 0.8750 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8334 - prc: 0.4018\n"
     ]
    }
   ],
   "source": [
    "loss, tp, fp, tn, fn, accuracy, precision, recall, auc, prc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2b06d5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.98      0.68       666\n",
      "           2       0.00      0.00      0.00       317\n",
      "           3       0.00      0.00      0.00       125\n",
      "           4       0.00      0.00      0.00        51\n",
      "           5       0.00      0.00      0.00        46\n",
      "           6       0.23      0.75      0.35         8\n",
      "           7       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.51      1282\n",
      "   macro avg       0.11      0.25      0.15      1282\n",
      "weighted avg       0.27      0.51      0.35      1282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classify pose in the TEST dataset using the trained model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the prediction result to class name\n",
    "y_pred_label = [class_names[i] for i in np.argmax(y_pred, axis=1)]\n",
    "y_true_label = [class_names[i] for i in np.argmax(y_test, axis=1)]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "plot_confusion_matrix(cm,\n",
    "                      class_names,\n",
    "                      title ='Confusion Matrix of Pose Classification Model')\n",
    "\n",
    "# Print the classification report\n",
    "print('\\nClassification Report:\\n', classification_report(y_true_label,\n",
    "                                                          y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c23041",
   "metadata": {},
   "source": [
    "## Normalize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "28566ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 51)]         0           []                               \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 17, 3)        0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 17, 2)       0           ['reshape_3[0][0]']              \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather (TFOpLambd  (None, 2)           0           ['tf.__operators__.getitem_3[0][0\n",
      " a)                                                              ]']                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_1 (TFOpLam  (None, 2)           0           ['tf.__operators__.getitem_3[0][0\n",
      " bda)                                                            ]']                              \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 2)            0           ['tf.compat.v1.gather[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_1[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 2)           0           ['tf.math.multiply[0][0]',       \n",
      " da)                                                              'tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.size (TFOpLambda)  ()                  0           ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1, 2)         0           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div (TFOpLa  ()                  0           ['tf.compat.v1.size[0][0]']      \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.broadcast_to (TFOpLambda)   (None, 17, 2)        0           ['tf.expand_dims[0][0]',         \n",
      "                                                                  'tf.compat.v1.floor_div[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, 17, 2)        0           ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.broadcast_to[0][0]']        \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_6 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_7 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_6[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_7[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 2)           0           ['tf.math.multiply_6[0][0]',     \n",
      " mbda)                                                            'tf.math.multiply_7[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.size_1 (TFOpLambd  ()                  0           ['tf.math.subtract[0][0]']       \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_4 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_5 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_2 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_3 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (None, 1, 2)         0           ['tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div_1 (TFOp  ()                  0           ['tf.compat.v1.size_1[0][0]']    \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_4[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_5[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_2[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.math.multiply_3 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_3[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.broadcast_to_1 (TFOpLambda)  (None, 17, 2)       0           ['tf.expand_dims_1[0][0]',       \n",
      "                                                                  'tf.compat.v1.floor_div_1[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 2)           0           ['tf.math.multiply_4[0][0]',     \n",
      " mbda)                                                            'tf.math.multiply_5[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 2)           0           ['tf.math.multiply_2[0][0]',     \n",
      " mbda)                                                            'tf.math.multiply_3[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.subtract_2 (TFOpLambda  (None, 17, 2)       0           ['tf.math.subtract[0][0]',       \n",
      " )                                                                'tf.broadcast_to_1[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.subtract_1 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_2[0][0]', \n",
      " )                                                                'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_8 (TFOpLam  (17, 2)             0           ['tf.math.subtract_2[0][0]']     \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm (TFOpLambda)  ()                  0           ['tf.math.subtract_1[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_1 (TFOpLambd  (2,)                0           ['tf.compat.v1.gather_8[0][0]']  \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLambda  ()                  0           ['tf.compat.v1.norm[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  ()                  0           ['tf.compat.v1.norm_1[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.maximum (TFOpLambda)   ()                   0           ['tf.math.multiply_8[0][0]',     \n",
      "                                                                  'tf.math.reduce_max[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv (TFOpLambda)   (None, 17, 2)        0           ['tf.math.subtract[0][0]',       \n",
      "                                                                  'tf.math.maximum[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 34)           0           ['tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 128)          4480        ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 128)          0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           8256        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 8)            520         ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,256\n",
      "Trainable params: 13,256\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "inputs = tf.keras.Input(shape=(51))\n",
    "embedding = landmarks_to_embedding(inputs)\n",
    "\n",
    "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7224c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "633/641 [============================>.] - ETA: 0s - loss: 0.1994 - accuracy: 0.6486\n",
      "Epoch 1: val_accuracy improved from -inf to 0.78315, saving model to weights.best.801010split\n",
      "INFO:tensorflow:Assets written to: weights.best.801010split/assets\n",
      "641/641 [==============================] - 4s 5ms/step - loss: 0.1993 - accuracy: 0.6489 - val_loss: 0.1273 - val_accuracy: 0.7832\n",
      "Epoch 2/200\n",
      "628/641 [============================>.] - ETA: 0s - loss: 0.1453 - accuracy: 0.7559\n",
      "Epoch 2: val_accuracy improved from 0.78315 to 0.79875, saving model to weights.best.801010split\n",
      "INFO:tensorflow:Assets written to: weights.best.801010split/assets\n",
      "641/641 [==============================] - 3s 4ms/step - loss: 0.1451 - accuracy: 0.7559 - val_loss: 0.1095 - val_accuracy: 0.7988\n",
      "Epoch 3/200\n",
      "629/641 [============================>.] - ETA: 0s - loss: 0.1318 - accuracy: 0.7783\n",
      "Epoch 3: val_accuracy improved from 0.79875 to 0.81669, saving model to weights.best.801010split\n",
      "INFO:tensorflow:Assets written to: weights.best.801010split/assets\n",
      "641/641 [==============================] - 3s 5ms/step - loss: 0.1318 - accuracy: 0.7783 - val_loss: 0.0997 - val_accuracy: 0.8167\n",
      "Epoch 4/200\n",
      "628/641 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.7855\n",
      "Epoch 4: val_accuracy improved from 0.81669 to 0.82917, saving model to weights.best.801010split\n",
      "INFO:tensorflow:Assets written to: weights.best.801010split/assets\n",
      "641/641 [==============================] - 3s 4ms/step - loss: 0.1246 - accuracy: 0.7847 - val_loss: 0.0980 - val_accuracy: 0.8292\n",
      "Epoch 5/200\n",
      "634/641 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.7943\n",
      "Epoch 5: val_accuracy improved from 0.82917 to 0.82995, saving model to weights.best.801010split\n",
      "INFO:tensorflow:Assets written to: weights.best.801010split/assets\n",
      "641/641 [==============================] - 3s 4ms/step - loss: 0.1194 - accuracy: 0.7949 - val_loss: 0.0914 - val_accuracy: 0.8300\n",
      "Epoch 6/200\n",
      "628/641 [============================>.] - ETA: 0s - loss: 0.1154 - accuracy: 0.7962\n",
      "Epoch 6: val_accuracy did not improve from 0.82995\n",
      "641/641 [==============================] - 1s 2ms/step - loss: 0.1151 - accuracy: 0.7969 - val_loss: 0.0949 - val_accuracy: 0.8292\n",
      "Epoch 7/200\n",
      "635/641 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.8016\n",
      "Epoch 7: val_accuracy improved from 0.82995 to 0.83853, saving model to weights.best.801010split\n",
      "INFO:tensorflow:Assets written to: weights.best.801010split/assets\n",
      "641/641 [==============================] - 3s 4ms/step - loss: 0.1141 - accuracy: 0.8014 - val_loss: 0.0875 - val_accuracy: 0.8385\n",
      "Epoch 8/200\n",
      "637/641 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.8042\n",
      "Epoch 8: val_accuracy improved from 0.83853 to 0.85179, saving model to weights.best.801010split\n",
      "INFO:tensorflow:Assets written to: weights.best.801010split/assets\n",
      "641/641 [==============================] - 3s 5ms/step - loss: 0.1110 - accuracy: 0.8040 - val_loss: 0.0849 - val_accuracy: 0.8518\n",
      "Epoch 9/200\n",
      "639/641 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.8119\n",
      "Epoch 9: val_accuracy did not improve from 0.85179\n",
      "641/641 [==============================] - 1s 2ms/step - loss: 0.1081 - accuracy: 0.8119 - val_loss: 0.0837 - val_accuracy: 0.8440\n",
      "Epoch 10/200\n",
      "629/641 [============================>.] - ETA: 0s - loss: 0.1058 - accuracy: 0.8126\n",
      "Epoch 10: val_accuracy improved from 0.85179 to 0.85569, saving model to weights.best.801010split\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tfa.losses.SigmoidFocalCrossEntropy(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Add a checkpoint callback to store the checkpoint that has the highest\n",
    "# validation accuracy.\n",
    "#checkpoint_path = \"weights.best.hdf5\"\n",
    "\n",
    "checkpoint_path = \"weights.best.801010split\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                              patience=20)\n",
    "\n",
    "# Start training\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training history to see whether you're overfitting.\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['TRAIN', 'VAL'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the TEST dataset\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7826a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify pose in the TEST dataset using the trained model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the prediction result to class name\n",
    "y_pred_label = [class_names[i] for i in np.argmax(y_pred, axis=1)]\n",
    "y_true_label = [class_names[i] for i in np.argmax(y_test, axis=1)]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "plot_confusion_matrix(cm,\n",
    "                      class_names,\n",
    "                      title ='Confusion Matrix of Pose Classification Model')\n",
    "\n",
    "# Print the classification report\n",
    "print('\\nClassification Report:\\n', classification_report(y_true_label,\n",
    "                                                          y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9c7d5",
   "metadata": {},
   "source": [
    "## No normalizing size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pose_landmarks_norescale(landmarks):\n",
    "    \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
    "        scaling it to a constant pose size.\n",
    "    \"\"\"\n",
    "    # Move landmarks so that the pose center becomes (0,0)\n",
    "    pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "    pose_center = tf.expand_dims(pose_center, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to perform\n",
    "    # substraction\n",
    "    pose_center = tf.broadcast_to(pose_center, \n",
    "                                [tf.size(landmarks) // (17*2), 17, 2])\n",
    "    landmarks = landmarks - pose_center\n",
    "\n",
    "    return landmarks\n",
    "def landmarks_to_embedding_norescale(landmarks_and_scores):\n",
    "    \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
    "    # Reshape the flat input into a matrix with shape=(17, 3)\n",
    "    reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
    "\n",
    "    # Normalize landmarks 2D\n",
    "    landmarks = normalize_pose_landmarks_norescale(reshaped_inputs[:, :, :2])\n",
    "\n",
    "    # Flatten the normalized landmark coordinates into a vector\n",
    "    embedding = keras.layers.Flatten()(landmarks)\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a575572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "inputs = tf.keras.Input(shape=(51))\n",
    "embedding = landmarks_to_embedding_norescale(inputs)\n",
    "\n",
    "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
    "\n",
    "model2 = keras.Model(inputs, outputs)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd62899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tfa.losses.SigmoidFocalCrossEntropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Add a checkpoint callback to store the checkpoint that has the highest\n",
    "# validation accuracy.\n",
    "checkpoint_path = \"weights.best.801010splitnonormsize\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                              patience=20)\n",
    "\n",
    "# Start training\n",
    "history2 = model2.fit(X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b965e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training history to see whether you're overfitting.\n",
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['TRAIN', 'VAL'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22cf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"Plots the confusion matrix.\"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=55)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "              horizontalalignment=\"center\",\n",
    "              color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Classify pose in the TEST dataset using the trained model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the prediction result to class name\n",
    "y_pred_label = [class_names[i] for i in np.argmax(y_pred, axis=1)]\n",
    "y_true_label = [class_names[i] for i in np.argmax(y_test, axis=1)]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "plot_confusion_matrix(cm,\n",
    "                      class_names,\n",
    "                      title ='Confusion Matrix of Pose Classification Model')\n",
    "\n",
    "# Print the classification report\n",
    "print('\\nClassification Report:\\n', classification_report(y_true_label,\n",
    "                                                          y_pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53bf5e9",
   "metadata": {},
   "source": [
    "## No normalizing position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pose_landmarks_noreposition(landmarks):\n",
    "    \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
    "        scaling it to a constant pose size.\n",
    "    \"\"\"\n",
    "   # Scale the landmarks to a constant pose size\n",
    "    pose_size = get_pose_size(landmarks)\n",
    "    landmarks /= pose_size\n",
    "\n",
    "    return landmarks\n",
    "def landmarks_to_embedding_norescale(landmarks_and_scores):\n",
    "    \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
    "    # Reshape the flat input into a matrix with shape=(17, 3)\n",
    "    reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
    "\n",
    "    # Normalize landmarks 2D\n",
    "    landmarks = normalize_pose_landmarks_noreposition(reshaped_inputs[:, :, :2])\n",
    "\n",
    "    # Flatten the normalized landmark coordinates into a vector\n",
    "    embedding = keras.layers.Flatten()(landmarks)\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "inputs = tf.keras.Input(shape=(51))\n",
    "embedding = landmarks_to_embedding_norescale(inputs)\n",
    "\n",
    "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
    "\n",
    "model2 = keras.Model(inputs, outputs)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tfa.losses.SigmoidFocalCrossEntropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Add a checkpoint callback to store the checkpoint that has the highest\n",
    "# validation accuracy.\n",
    "checkpoint_path = \"weights.best.801010splitnonormsize\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                              patience=20)\n",
    "\n",
    "# Start training\n",
    "history2 = model2.fit(X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training history to see whether you're overfitting.\n",
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['TRAIN', 'VAL'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99261110",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977451df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify pose in the TEST dataset using the trained model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the prediction result to class name\n",
    "y_pred_label = [class_names[i] for i in np.argmax(y_pred, axis=1)]\n",
    "y_true_label = [class_names[i] for i in np.argmax(y_test, axis=1)]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "plot_confusion_matrix(cm,\n",
    "                      class_names,\n",
    "                      title ='Confusion Matrix of Pose Classification Model')\n",
    "\n",
    "# Print the classification report\n",
    "print('\\nClassification Report:\\n', classification_report(y_true_label,\n",
    "                                                          y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436bd7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354390fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
